# Transformer

## ğŸ§¾ Tá»•ng quan
**Transformer** khÃ´ng chá»‰ cÃ³ kháº£ nÄƒng mÃ´ hÃ¬nh hÃ³a sÃ¢u (deep modeling) vÃ  kháº£ nÄƒng má»Ÿ rá»™ng (scalability), mÃ  cÃ²n thá»ƒ hiá»‡n **kháº£ nÄƒng thÃ­ch á»©ng máº¡nh máº½ vá»›i nhiá»u modality khÃ¡c nhau**.

Trong cÃ¡c nhiá»‡m vá»¥ Ä‘a phÆ°Æ¡ng thá»©c, cháº³ng háº¡n nhÆ° **Text-based Person Re-ID**, Transformer Ä‘Ã£ trá»Ÿ thÃ nh **ná»n táº£ng cÃ´ng nghá»‡ quan trá»ng**, nhá» vÃ o kháº£ nÄƒng **trÃ­ch xuáº¥t vÃ  cÄƒn chá»‰nh hiá»‡u quáº£ cÃ¡c Ä‘áº·c trÆ°ng vÄƒn báº£n vÃ  hÃ¬nh áº£nh**. [77, 82, 119, 120, 78, 8, 99, 84, 118, 98, 90, 79, 2, 85, 34, 114, 121, 37, 122, 35, 38, 39, 40, 41, 42, 43, 44, 123]

Theo **cÃ¡ch mÃ´ hÃ¬nh xá»­ lÃ½ vÄƒn báº£n vÃ  hÃ¬nh áº£nh**, cÃ¡c framework cÃ´ng nghá»‡ hiá»‡n nay cÃ³ thá»ƒ chia thÃ nh **hai loáº¡i chÃ­nh**:
- Dual-stream Encoding
- Unified Large Models Era Encoding

## Dual-stream Encoding
Dual-stream Encoding sá»­ dá»¥ng cÃ¡c **bá»™ mÃ£ hÃ³a Ä‘á»™c láº­p cho vÄƒn báº£n vÃ  hÃ¬nh áº£nh** Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« cáº£ hai modality.
- Má»—i bá»™ mÃ£ hÃ³a (vÃ­ dá»¥: BERT, Swin Transformer) táº­p trung vÃ o kiá»ƒu dá»¯ liá»‡u riÃªng cá»§a nÃ³:
    - VÄƒn báº£n: sá»­ dá»¥ng Transformer [94] vÃ  BERT [8, 99, 84, 118, 98, 90, 79, 2, 85, 34, 114, 121, 122, 45] Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng ngá»¯ cáº£nh.
    - HÃ¬nh áº£nh: sá»­ dá»¥ng Vision Transformer [124, 121], Swin Transformer [78, 37, 45], Pyramid ViT [85]... Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng Ä‘a tá»‰ lá»‡ tá»« áº£nh ngÆ°á»i.

Sau Ä‘Ã³, Ä‘áº·c trÆ°ng cá»§a cáº£ hai modality Ä‘Æ°á»£c **chiáº¿u vÃ o cÃ¹ng má»™t khÃ´ng gian** Ä‘á»ƒ so khá»›p thÃ´ng qua má»™t cÆ¡ cháº¿ cÄƒn chá»‰nh Ä‘áº·c thÃ¹ (vÃ­ dá»¥: Contrastive Learning).

### Má»™t sá»‘ cÃ´ng trÃ¬nh tiÃªu biá»ƒu
- **Li et al.** [119]: Ä‘á» xuáº¥t máº¡ng **SAF (Semantically-Aligned Feature aggregation)** dá»±a trÃªn Vision Transformer, cÃ³ kháº£ nÄƒng gom cÃ¡c Ä‘áº·c trÆ°ng cÃ³ cÃ¹ng ngá»¯ nghÄ©a thÃ nh nhá»¯ng Ä‘áº·c trÆ°ng cá»¥c bá»™ riÃªng biá»‡t.
- **Ji et al.** [78]: Ä‘Æ°a ra phÆ°Æ¡ng phÃ¡p **Asymmetric Cross-Scale Alignment (ACSA)** dÃ¹ng Swin Transformer Ä‘á»ƒ cÄƒn chá»‰nh toÃ n cá»¥c giá»¯a hÃ¬nh áº£nh vÃ  vÄƒn báº£n, sau Ä‘Ã³ cÄƒn chá»‰nh Ä‘á»™ng cÃ¡c thá»±c thá»ƒ cross-modal, giáº£i quyáº¿t váº¥n Ä‘á» lá»‡ch tá»‰ lá»‡ (scale alignment).
- **Shao et al.** [118]: Ä‘á» xuáº¥t **LGUR (Learning Granularity-Unified Representations)** dá»±a trÃªn DeiT-Transformer, Ã¡nh xáº¡ Ä‘áº·c trÆ°ng hÃ¬nh áº£nh vÃ  vÄƒn báº£n vÃ o má»™t khÃ´ng gian thá»‘ng nháº¥t vá» má»©c Ä‘á»™ chi tiáº¿t (granularity).
- **Ref.** [85]: xÃ¢y dá»±ng khung tokenization vÃ  cÄƒn chá»‰nh Ä‘áº·c trÆ°ng dá»±a trÃªn Pyramid ViT, giÃºp dáº§n thu háº¹p khoáº£ng cÃ¡ch giá»¯a cÃ¡c modality vÃ  giá»¯a cÃ¡c lá»›p (inter/intra-class).
- **Li et al.** [121]: phÃ¡t triá»ƒn mÃ´ hÃ¬nh **MSN-BRR (Multi-Granularity Separation Network with Bidirectional Refinement Regularization)**, chá»‰ dÃ¹ng BERT Ä‘á»ƒ mÃ£ hÃ³a vÄƒn báº£n, táº­p trung xá»­ lÃ½ nhiá»…u liÃªn-modal.
- **Yang et al.** [37]: Ä‘á» xuáº¥t **APTM (Attribute Prompt Learning and Text Matching Learning)** dá»±a trÃªn Swin Transformer, bá»• sung explicit attribute recognition Ä‘á»ƒ cáº£i thiá»‡n representation learning.
- **Ref.** [122]: khai thÃ¡c thÃ´ng tin bá»‹ bá» sÃ³t báº±ng phÆ°Æ¡ng phÃ¡p dá»±a trÃªn Vision Transformer, káº¿t há»£p khai thÃ¡c Ä‘áº·c trÆ°ng tiáº¿n trÃ¬nh vÃ  tri thá»©c ngoÃ i Ä‘á»ƒ tinh lá»c Ä‘áº·c trÆ°ng.
- **Li et al.** [45]: giáº£i quyáº¿t váº¥n Ä‘á» khoáº£ng cÃ¡ch liÃªn-modal vÃ  biáº¿n thiÃªn ná»™i/ngoáº¡i lá»›p báº±ng khung **AUL (Adaptive Uncertainty Based Learning)** dá»±a trÃªn Swin Transformer.
- **Shu et al.** [94]: Ä‘á» xuáº¥t khung **IVT (Implicit Visual Text)**, trong Ä‘Ã³ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng vÄƒn báº£n vÃ  hÃ¬nh áº£nh dÃ¹ng chung má»™t backbone Transformer (chia sáº» tham sá»‘). CÃ¡ch nÃ y, tá»‘i Æ°u báº±ng dá»¯ liá»‡u cross-modal, cho phÃ©p mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c má»™t Ã¡nh xáº¡ khÃ´ng gian chung.